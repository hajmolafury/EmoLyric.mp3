{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import sys\nimport textwrap\n \n \nprint(sys.version_info)\nif sys.version_info.major == 2:\n    eider.experimental.pip_import(\"backports.weakref\")\n    # major hackery in the line below. still working through issues installing tensorflow with python 2.7 on Eider\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-2.7 install --user --upgrade tensorflow\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-2.7 install \"https://files.pythonhosted.org/packages/d5/98/e2e9d5afbc86cef0b2dd0f4ab791519b9bd305ea207e1e5c2f9a9f2f6da6/tensorboard-1.9.0-py2-none-any.whl\" --user -q\n    print(textwrap.dedent(\"\"\"\n        WARNING: After completion of the install of tensorflow, there are issues when using Python 2.7 in Eider.\n        \n        There is similar behavior exhibited in Jupyter:\n        \n        * https://github.com/GoogleCloudPlatform/training-data-analyst/issues/133\n        * https://aichamp.wordpress.com/2016/11/13/handeling-importerror-cannot-import-name-pywrap_tensorflow/\n        \n        You can get around this issue by restarting your Eider worker process via the System > Restart menu in Eider\n        and updating sys.path to include the site.USER_SITE directory with the already-installed tensorflow package.\n    \"\"\"))\nelse:\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-3.4 install --user --upgrade tensorflow\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-3.4 install \"https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl\" --user -q" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "eider.experimental._include_user_site()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import time\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import InputLayer, Input\nfrom tensorflow.python.keras.layers import Reshape, MaxPooling2D\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from boto.s3.connection import S3Connection\nconn = S3Connection('AKIAJPXSDB77ADEXSAAQ','bs+D8l43z/2rnvQQoq6s50EZL70efoT2pqgpG/Ff')\nbucket = conn.get_bucket('lyrics-ml-hackathon')\n# for object in bucket.list(prefix='train_mfcc/'):\n#     print(object.key)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import boto3\nimport binascii\nimport numpy as np\nfrom io import BytesIO\ndef read_file(filename):\n    f=open(filename, 'rb')\n    data = f.read()\n    f.close()\n    return data\ns3 = boto3.resource('s3',aws_access_key_id='AKIAJPXSDB77ADEXSAAQ',aws_secret_access_key='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')\nclient = boto3.client('s3', aws_access_key_id='AKIAJPXSDB77ADEXSAAQ',aws_secret_access_key='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')\nbucket=s3.Bucket('lyrics-ml-hackathon')\nprefix_train = \"train_mfcc/\"\nx_train=[]\ny_train=[]\n\ndata = [obj for obj in list(bucket.objects.filter(Prefix=prefix_train)) if obj.key != prefix_train]\nfor obj1 in data[:]:\n    obj = client.get_object(Bucket='lyrics-ml-hackathon', Key=obj1.key)\n    #print(obj1.key)\n    x_train.append(np.frombuffer(obj['Body'].read()))\n    y_train.append(obj1.key.split(\"/\")[1].split(\"_\")[-1].strip(\".txt\"))\n   \n    " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "x_test=[]\ny_test=[]\n\nprefix_test = \"emotion-recognition-236f22a6fde0_validation/\"  \ndata = [obj for obj in list(bucket.objects.filter(Prefix=prefix_test)) if obj.key != prefix_test]\nfor obj1 in data[:]:\n    obj = client.get_object(Bucket='lyrics-ml-hackathon', Key=obj1.key)\n    x_test.append(np.frombuffer(obj['Body'].read()))\n    y_test.append(obj1.key.split(\"/\")[1].split(\"_\")[-1].strip(\".txt\"))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "y_train=[int(i) for i in y_train]\ny_test=[int(i) for i in y_test]" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# nodes_hl1=256\n# nodes_hl2=256\n# n_classes=4\n# batch_size=64\n# train_examples=len(x_train)\n# test_examples=len(x_test)\n# x_train_np = np.asarray(x_train)\n# x_test_np = np.asarray(x_test)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# x_train=tf.Session().run(tf.reshape(x_train_np, [train_examples,40]))\n# x_test=tf.Session().run(tf.reshape(x_test_np, [test_examples,40]))\n\n# y_train_1hot=tf.Session().run(tf.reshape(tf.Session().run(tf.one_hot(y_train,depth=n_classes)),[train_examples,n_classes]))\n# y_test_1hot=tf.Session().run(tf.reshape(tf.Session().run(tf.one_hot(y_test,depth=n_classes)),[test_examples,n_classes]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# x=tf.placeholder('float',[None,40])\n# y=tf.placeholder('float',[None, n_classes])" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# def neural_network(x_data):\n#     initializer= tf.contrib.layers.xavier_initializer()\n#     hl1={'weights':tf.Variable(initializer([40,nodes_hl1])),'bias':tf.Variable(initializer([nodes_hl1]))}\n#     hl2={'weights':tf.Variable(initializer([nodes_hl1,nodes_hl2])),'bias':tf.Variable(initializer([nodes_hl2]))}\n#     output_layer={'weights':tf.Variable(initializer([nodes_hl2,n_classes])),'bias':tf.Variable(initializer([n_classes]))}\n    \n#     l1=tf.nn.relu(tf.add(tf.matmul(x_data,hl1['weights']),hl1['bias']))            \n#     l2=tf.nn.relu(tf.add(tf.matmul(l1,hl2['weights']),hl2['bias']))\n#     output=tf.nn.sigmoid(tf.add(tf.matmul(l2,output_layer['weights']),output_layer['bias']))                                                           \n#     return output" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# def train_neural_network():\n#     prediction=neural_network(x)\n#     cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n    \n#     optimize=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n        \n#     with tf.Session() as sess:\n#         sess.run(tf.global_variables_initializer())\n        \n#         for epoch in range(num_epoch):\n#             for i in range(0, train_examples, batch_size):\n#                 batch_x = x_train[i:i + batch_size]\n#                 batch_y = y_train_1hot[i:i + batch_size]\n#                 _, l=sess.run([optimize, cost],feed_dict={x:batch_x,y:batch_y})\n#                 epoch_loss[epoch]+=l\n            \n#             correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n#             accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n#             test_acc.append(accuracy.eval({x:x_test,y:y_test_1hot}))\n    \n#             print ('Epoch', epoch+1,'completed out of',num_epoch,'loss:',epoch_loss[epoch])            \n#             print ('Test Accuracy :', test_acc[-1])\n#             print (time.time() - start_time, 'sec')" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# num_epoch=200\n# epoch_loss=np.zeros(num_epoch)\n# test_acc=[]\n\n# if __name__==\"__main__\":\n#     start_time=time.time()\n#     train_neural_network()\n#     print (\"EmoLyric has been trained!\")  " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling1D" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "x_train2=tf.Session().run(tf.reshape(np.array(x_train), [2000,40,1]))\nx_test2=tf.Session().run(tf.reshape(np.array(x_test), [906,40,1]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# y_train2=tf.Session().run(tf.reshape(np.asarray(y_train), [2000,1]))\n# y_test2=tf.Session().run(tf.reshape(np.asarray(y_test), [906,40]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "y_train2=tf.Session().run(tf.reshape(tf.Session().run(tf.one_hot(y_train,depth=4)),[2000,4]))\ny_test2=tf.Session().run(tf.reshape(tf.Session().run(tf.one_hot(y_test,depth=4)),[906,4]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "model = Sequential()\nmodel.add(Conv1D(128, 5, activation='relu', input_shape=(40,1)))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4, activation='softmax'))\n\nprint(model.summary())\nadamV2=tf.keras.optimizers.Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\nmodel.compile(optimizer=adamV2, loss='categorical_crossentropy', metrics=['acc'])" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "model.fit(x_train2, y_train2, epochs=100, verbose=2, validation_data=(x_test2, y_test2))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "loss, accuracy = model.evaluate(x_test2, y_test2, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "amazon_data=[]\n\nprefix_test = \"mfcc_amazon_100/\"  \ndata = [obj for obj in list(bucket.objects.filter(Prefix=prefix_test)) if obj.key != prefix_test]\nfor obj1 in data[:]:\n    obj = client.get_object(Bucket='lyrics-ml-hackathon', Key=obj1.key)\n    amazon_data.append(np.frombuffer(obj['Body'].read()))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "amazon_data2=tf.Session().run(tf.reshape(np.array(amazon_data), [100,40,1]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "for i in len(\na.index(max(a))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "amazon_data_prediction = model.predict(amazon_data2, verbose=1)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "amazon_data_prediction" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "np.savetxt(\"/tmp/amazon_data_prediction.csv\", amazon_data_prediction, delimiter=\",\")" ]
  } ]
}
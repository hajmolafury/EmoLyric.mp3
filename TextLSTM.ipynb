{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import sys\nimport textwrap\n\nprint(sys.version_info)\nif sys.version_info.major == 2:\n    eider.experimental.pip_import(\"backports.weakref\")\n    # major hackery in the line below. still working through issues installing tensorflow with python 2.7 on Eider\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-2.7 install --user --upgrade tensorflow\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-2.7 install \"https://files.pythonhosted.org/packages/d5/98/e2e9d5afbc86cef0b2dd0f4ab791519b9bd305ea207e1e5c2f9a9f2f6da6/tensorboard-1.9.0-py2-none-any.whl\" --user -q\n    print(textwrap.dedent(\"\"\"\n        WARNING: After completion of the install of tensorflow, there are issues when using Python 2.7 in Eider.\n        \n        There is similar behavior exhibited in Jupyter:\n        \n        * https://github.com/GoogleCloudPlatform/training-data-analyst/issues/133\n        * https://aichamp.wordpress.com/2016/11/13/handeling-importerror-cannot-import-name-pywrap_tensorflow/\n        \n        You can get around this issue by restarting your Eider worker process via the System > Restart menu in Eider\n        and updating sys.path to include the site.USER_SITE directory with the already-installed tensorflow package.\n    \"\"\"))\nelse:\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-3.4 install --user --upgrade tensorflow\n    !env -i PATH=\"/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:$PATH\" /usr/bin/pip-3.4 install \"https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl\" --user -q" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "eider.experimental._include_user_site()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import time\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import InputLayer, Input\nfrom tensorflow.python.keras.layers import Reshape\nfrom tensorflow.python.keras.layers import Dense, Flatten" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from boto.s3.connection import S3Connection\nconn = S3Connection('AKIAJPXSDB77ADEXSAAQ','bs+D8l43z/2rnvQQoq6s50EZL70efoT2pqgpG/Ff')\nbucket = conn.get_bucket('lyrics-ml-hackathon')" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "#need to get data\nimport boto3\nclient = boto3.client('s3', aws_access_key_id='AKIAJPXSDB77ADEXSAAQ',aws_secret_access_key='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')\nobj = client.get_object(Bucket='lyrics-ml-hackathon', Key='output_moods_300d.txt')\nobj2 = client.get_object(Bucket='lyrics-ml-hackathon', Key='output_moods_genius_300d.txt')\ndata_dump = obj['Body'].read()\ndata_dump2=obj2['Body'].read()\nlines1 = data_dump.split('\\n')\nlines2 = data_dump2.split('\\n')\nall_lines = lines1 + lines2\n\ntext = []\nlabels = []\nfor each_line in all_lines:\n    each_line_sp = each_line.split('\\t')\n    lx = each_line_sp[1].split(',')\n    if not lx: continue\n    temp_str=\"\"\n    for i in range(len(lx)):\n        if i > 99:\n            break;\n        temp_str+=lx[i]\n        temp_str+=\" \" \n    if len(each_line_sp[0]) > 1:\n        continue\n    text.append(temp_str)\n    labels.append(int(each_line_sp[0]))\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "MAX_SEQUENCE_LENGTH=100\nVALIDATION_SPLIT=0.2\nEMBEDDING_DIM=300" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras import utils as np_utils\nfrom keras import backend as K\nimport os\n\n\ndef set_keras_backend(backend):\n\n    if K.backend() != backend:\n        os.environ['KERAS_BACKEND'] = backend\n        reload(K)\n        assert K.backend() == backend\n\nset_keras_backend(\"tensorflow\")" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "tokenizer = Tokenizer()\ntokenizer.fit_on_texts(text)\nsequences = tokenizer.texts_to_sequences(text)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nlabels_cat = to_categorical(labels,num_classes=4)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels_cat = labels_cat[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels_cat[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels_cat[-nb_validation_samples:]" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "unique, counts = np.unique(labels, return_counts=True)\ndict(zip(unique, counts))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print('Shape of data tensor:', x_val.shape)\nprint('Shape of label tensor:', y_val.shape)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print data[0]" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import boto3\nclient = boto3.client('s3', aws_access_key_id='AKIAJPXSDB77ADEXSAAQ',aws_secret_access_key='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')\nembeddings_index = {}\nobj = client.get_object(Bucket='lyrics-ml-hackathon', Key='glove300d_moods_filtered.txt')\nobj2 = client.get_object(Bucket='lyrics-ml-hackathon', Key='glove300d_moods_genius_filtered.txt')\n\ndata_glove = obj['Body'].read()\nfor line in data_glove.split('\\n'):\n    values = line.split()\n    if len(values) == 0:\n        continue;\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n# print('Found %s word vectors.' % len(embeddings_index))\n\ndata_glove2 = obj2['Body'].read()\nfor line in data_glove2.split('\\n'):\n    values = line.split()\n    if len(values) == 0:\n        continue;\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    if word in embeddings_index.keys():\n        continue\n    else:\n        embeddings_index[word] = coefs" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "embeddings_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import TimeDistributed" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "embedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embeddings_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(30))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))\nprint(model.summary())\n\nadamV2=tf.keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\nmodel.compile(optimizer=adamV2, loss='categorical_crossentropy', metrics=['acc'])" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "model.fit(data, labels_cat, batch_size=64,epochs=50, verbose=2, validation_split=0.2)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "output =model.predict_classes(data, verbose=2)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "unique, counts = np.unique(output, return_counts=True)\ndict(zip(unique, counts))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n# embedded_sequences = embedding_layer(sequence_input)\n# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n# x = MaxPooling1D(5,padding='same')(x)\n# x = Conv1D(128, 5, activation='relu')(x)\n# x = MaxPooling1D(5)(x)\n# x = Conv1D(128, 5, activation='relu')(x)\n# x = MaxPooling1D(35)(x)\n# x = Flatten()(x)\n# x = Dense(128, activation='relu')(x)\npreds = Dense(4, activation='softmax')(x)\nmodel = Model(sequence_input, preds)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test),\n          epochs=2, batch_size=64)" ]
  } ]
}